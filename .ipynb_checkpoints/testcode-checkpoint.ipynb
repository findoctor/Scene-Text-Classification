{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from scipy.io import loadmat\n",
    "annots = loadmat('Data/IIIT5K/traindata.mat')\n",
    "annots['traindata'][0][100][1][0]\n",
    "# second para: row . Extract name\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "'''\n",
    "\n",
    "import os.path\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "#from warpctc_pytorch import CTCLoss\n",
    "import os\n",
    "#import dataset\n",
    "from dataset import lmdbDataset, resizeNormalize, randomSequentialSampler, alignCollate\n",
    "import create_dataset\n",
    "\n",
    "\n",
    "lmdb_path = \"Data/lmdb\"\n",
    "train_dataset = lmdbDataset(root=lmdb_path)  # root to dataset\n",
    "assert train_dataset\n",
    "\n",
    "#sampler = randomSequentialSampler(train_dataset, 100)  # batch size = 100\n",
    "sampler = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100,\n",
    "    shuffle=True, sampler=sampler,\n",
    "    num_workers=4,\n",
    "    collate_fn=alignCollate(imgH=32, imgW=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_DataLoaderIter' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-36c00826aa2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '_DataLoaderIter' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, texts = train_iter.next()\n",
    "images2, texts2 = train_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['A', 'PIYO', 'SERVECIES', 'BLOCK', 'PEAS', 'AVENUE', '1025', 'FAMILY', 'WALK', 'VIEW', 'HOW', 'MADE', 'TO', 'FERRIS', 'THE', 'YOUR', 'YOUR', 'SERVE', 'RIVER', 'DELPY', 'THE', 'APTECH', 'CHARTS', 'SCHEMES', 'AHEAD', 'DURBAN', 'INDIA', 'COMPARISON', 'EVERYWHERE', 'AVAILABLE', 'SBI', 'SIDE', 'PLEASE', 'SAFE', 'STAIRS', 'SCREAM', '21', 'SPACE', 'OFFICE', 'KMPH', '3A', 'HILLS', '1877', 'ISNT', 'FEY', 'PPINANG', 'SANTA', 'A', 'THREE', 'END', 'KEANU', 'PRIME', 'USUAL', 'KHAIMAH', 'MAIDEN', 'BALE', 'TURN', '570', 'CU', 'HOARDING', '4', 'GEOFFREY', '165', 'BEECHES', 'DANGER', 'LEFT', 'BILLBOARD', 'INCEPTION', 'ALWAYS', '1', '39', 'THE', '34', 'COMPASSIONATE', 'EFFECTIVELY', 'COSY', 'COM', 'UP', 'YOUR', 'ECO', 'ADDRESS', 'WHY', 'WERE', 'ON', 'RETAIL', '49A', 'DEPOSIT', 'ASTAIRE', 'TO', 'STALLONE', 'CRESTDOWN', 'ADVERTISING', 'SAM', 'DAIRY', 'MADE', 'ONCE', 'DEAD', 'FRANKLIN', '1800BUYKWIK', 'GET']\n",
      "(\"b'A'\", \"b'PIYO'\", \"b'SERVECIES'\", \"b'BLOCK'\", \"b'PEAS'\", \"b'AVENUE'\", \"b'1025'\", \"b'FAMILY'\", \"b'WALK'\", \"b'VIEW'\", \"b'HOW'\", \"b'MADE'\", \"b'TO'\", \"b'FERRIS'\", \"b'THE'\", \"b'YOUR'\", \"b'YOUR'\", \"b'SERVE'\", \"b'RIVER'\", \"b'DELPY'\", \"b'THE'\", \"b'APTECH'\", \"b'CHARTS'\", \"b'SCHEMES'\", \"b'AHEAD'\", \"b'DURBAN'\", \"b'INDIA'\", \"b'COMPARISON'\", \"b'EVERYWHERE'\", \"b'AVAILABLE'\", \"b'SBI'\", \"b'SIDE'\", \"b'PLEASE'\", \"b'SAFE'\", \"b'STAIRS'\", \"b'SCREAM'\", \"b'21'\", \"b'SPACE'\", \"b'OFFICE'\", \"b'KMPH'\", \"b'3A'\", \"b'HILLS'\", \"b'1877'\", \"b'ISNT'\", \"b'FEY'\", \"b'PPINANG'\", \"b'SANTA'\", \"b'A'\", \"b'THREE'\", \"b'END'\", \"b'KEANU'\", \"b'PRIME'\", \"b'USUAL'\", \"b'KHAIMAH'\", \"b'MAIDEN'\", \"b'BALE'\", \"b'TURN'\", \"b'570'\", \"b'CU'\", \"b'HOARDING'\", \"b'4'\", \"b'GEOFFREY'\", \"b'165'\", \"b'BEECHES'\", \"b'DANGER'\", \"b'LEFT'\", \"b'BILLBOARD'\", \"b'INCEPTION'\", \"b'ALWAYS'\", \"b'1'\", \"b'39'\", \"b'THE'\", \"b'34'\", \"b'COMPASSIONATE'\", \"b'EFFECTIVELY'\", \"b'COSY'\", \"b'COM'\", \"b'UP'\", \"b'YOUR'\", \"b'ECO'\", \"b'ADDRESS'\", \"b'WHY'\", \"b'WERE'\", \"b'ON'\", \"b'RETAIL'\", \"b'49A'\", \"b'DEPOSIT'\", \"b'ASTAIRE'\", \"b'TO'\", \"b'STALLONE'\", \"b'CRESTDOWN'\", \"b'ADVERTISING'\", \"b'SAM'\", \"b'DAIRY'\", \"b'MADE'\", \"b'ONCE'\", \"b'DEAD'\", \"b'FRANKLIN'\", \"b'1800BUYKWIK'\", \"b'GET'\")\n"
     ]
    }
   ],
   "source": [
    "# texts is tuple, each element is a string label. 100 in total\n",
    "import collections\n",
    "print(isinstance(texts, collections.Iterable))\n",
    "new_texts = [s[2:-1] for s in texts]\n",
    "print(new_texts)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 32, 100])\n"
     ]
    }
   ],
   "source": [
    "# check batch image size\n",
    "print(images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert image and label to Torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine all image to wanted size: (32,100)\n",
    "# return v\n",
    "def refine_img(v, img):\n",
    "    v.data.resize_(img.size()).copy_(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store batch image and text in Tensor\n",
    "image = torch.FloatTensor(100, 1, 32, 100)  # 100: batch size  imgH, imgW\n",
    "text = torch.IntTensor(100 * 5)\n",
    "batch_size = torch.IntTensor(100)\n",
    "\n",
    "image = Variable(image)\n",
    "text = Variable(text)\n",
    "batch_size = Variable(batch_size)\n",
    "\n",
    "# TEST\n",
    "refine_img(image, images)  # images: read from train_iterator\n",
    "#print(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to encode and decode the label\n",
    "# Example:\n",
    "# Encode: 7abc --> [7,10,11,12]  (IntTensor)\n",
    "# Docode: [11,12,13] --> [b,c,d]\n",
    "# *****TODO*****\n",
    "# wrap all functions in a CLASS\n",
    "\n",
    "# Define alphabet in config.py\n",
    "alphabet = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "class LabelConverter(object):\n",
    "    def __init__(self, alphabet, ignore_case = True):\n",
    "        self.ignore_case = ignore_case\n",
    "        if self.ignore_case:\n",
    "            alphabet = alphabet.lower()\n",
    "        self.alphabet = alphabet + '-'\n",
    "        self.dict = {}\n",
    "        for index, c in enumerate(self.alphabet):\n",
    "            self.dict[c] = index + 1  # index 0 reserved for blank\n",
    "    \n",
    "    def encode(self, labels):\n",
    "        # input: labels: list of labels name\n",
    "        # return: [labels encoded], [labels length]\n",
    "        length = [len(c) for c in labels]\n",
    "        text = ''.join(labels)\n",
    "        encoded = []\n",
    "        for c in text:\n",
    "            encoded.append(self.dict[c.lower()] )\n",
    "        return torch.IntTensor(encoded), torch.IntTensor(length)\n",
    "    \n",
    "    def decode(self, text, length):\n",
    "        # decode to strs, batch mode\n",
    "        # text: list of encodings    length: list of length\n",
    "        assert sum(length) == text.numel()  # .numel used to calculate number of elements in text\n",
    "        decodings = []\n",
    "        pos = 0\n",
    "        for str_len in length:\n",
    "            encode = text[pos:pos+str_len]\n",
    "            decode = ''\n",
    "            for digit in encode:\n",
    "                decode += self.alphabet[digit-1]\n",
    "            decodings.append(decode)\n",
    "            pos+= str_len\n",
    "        return decodings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlabelConverter = LabelConverter(alphabet)\\nencode_label, encode_length = labelConverter.encode(new_texts)\\ndecodings = labelConverter.decode(encode_label, encode_length)\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "'''\n",
    "labelConverter = LabelConverter(alphabet)\n",
    "encode_label, encode_length = labelConverter.encode(new_texts)\n",
    "decodings = labelConverter.decode(encode_label, encode_length)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Manage Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new a network object and initialize its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class RCNN definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, imgW, num_classes=36, init_weights=True):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cfg = [64, 'M22', 128, 'M22', 256, 256, 'M12', 512, 'bc', 512, 'bc', 'M12', 512]\n",
    "        self.out_channels = [64, 128, 256, 256, 512, 512, 512]  # output channels of each conv layers\n",
    "        self.layers = self.make_layers(self.cfg)\n",
    "        \n",
    "    def make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 1  # in this example, channel is 1\n",
    "        layer_count = -1  # count conv layers\n",
    "        for v in cfg:\n",
    "            if v == 'M22':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif v == 'M12':\n",
    "                layers += [nn.MaxPool2d(kernel_size=(1,2), stride=2)]\n",
    "            elif v == 'bc':\n",
    "                layers += [nn.BatchNorm2d(self.out_channels[layer_count]), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                layer_count += 1\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST network\n",
    "crnn = CRNN(32, 100, 36)\n",
    "output = crnn(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 512, 2, 6])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable builtin_function_or_method object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-cc654c9765e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mcrnn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCRNN2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrnn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable builtin_function_or_method object"
     ]
    }
   ],
   "source": [
    "class CRNN2(nn.Module):\n",
    "\n",
    "    def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False):\n",
    "        super(CRNN2, self).__init__()\n",
    "        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        self.cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nc if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            self.cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                self.cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                self.cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                self.cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        self.cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        self.cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        self.cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        self.cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        conv = self.cnn(input)\n",
    "        \n",
    "        b, c, h, w = conv.size()\n",
    "        print(b,c,h,w)\n",
    "        '''\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        '''\n",
    "        return conv\n",
    "             \n",
    "\n",
    "# TEST network\n",
    "crnn2 = CRNN2(32, 1, 36, 2)\n",
    "output2 = crnn2(images)\n",
    "b, c, h, w = output2.size\n",
    "print(b,c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
