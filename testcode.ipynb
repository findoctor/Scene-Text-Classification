{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from scipy.io import loadmat\n",
    "annots = loadmat('Data/IIIT5K/traindata.mat')\n",
    "annots['traindata'][0][100][1][0]\n",
    "# second para: row . Extract name\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "'''\n",
    "\n",
    "import os.path\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "#from warpctc_pytorch import CTCLoss\n",
    "import os\n",
    "#import dataset\n",
    "from dataset import lmdbDataset, resizeNormalize, randomSequentialSampler, alignCollate\n",
    "import create_dataset\n",
    "\n",
    "\n",
    "lmdb_path = \"Data/lmdb\"\n",
    "train_dataset = lmdbDataset(root=lmdb_path)  # root to dataset\n",
    "assert train_dataset\n",
    "\n",
    "#sampler = randomSequentialSampler(train_dataset, 100)  # batch size = 100\n",
    "sampler = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=100,\n",
    "    shuffle=True, sampler=sampler,\n",
    "    num_workers=4,\n",
    "    collate_fn=alignCollate(imgH=32, imgW=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, texts = train_iter.next()\n",
    "images2, texts2 = train_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['THEYRE', 'SPACE', 'BANSAL', 'LANZMANN', 'EXCLUSIVE', 'AND', '1', '23', 'KAUSHAL', 'HAMMER', 'SAVE', 'WWWWEBSCOM', 'KNIGHT', 'RESULTS', 'TAYLOR', 'STATE', 'SIP', 'LANE', 'FUTURE', 'FROM', 'CRESTDOWN', 'REMITTANCES', 'THE', 'FAX', 'RECRUITMENT', '37', '12530', 'FANTASTISK', 'DR', 'INDIA', 'GC', 'FERRABY', 'DOGS', 'EVERY', 'PERSEEL', 'YOUR', 'VILLA', 'CAMPAIGN', 'FARM', 'ALL', '5', 'FOX', 'BANK', 'HOFFMAN', 'AS', 'MART', 'ROZOWEJ', 'SDN', 'ZEROEMISSIONS', 'GREM', 'FOR', 'DEPOSITPHOTOS', '1300', 'WILT', 'CAREER', '360', 'COPIES', 'COPY', 'OF', 'TRINITYLEEDS', 'EN', 'UP', 'ONGC', 'DANGERS', 'CAUTION', 'REPORT', 'CANNIESBURN', 'NEW', 'MADE', 'MEDICAL', 'KIRKLISTON', 'PERSONAL', 'LIVING', 'FRANKLIN', 'HOW', '8401950', 'END', 'ANDRIEJ', 'DO', 'KEENER', 'THEM', 'NAME', 'LANE', 'TRACK', 'CHRYSLER', 'SCHWARZENEGGER', 'ONE', 'ALL', '1306', 'BRASSERIE', 'OR', 'WHEN', 'ROADS', 'INDIA', 'THROUGHOUT', 'DO', '8', '2', 'DAY', '137']\n",
      "(\"b'THEYRE'\", \"b'SPACE'\", \"b'BANSAL'\", \"b'LANZMANN'\", \"b'EXCLUSIVE'\", \"b'AND'\", \"b'1'\", \"b'23'\", \"b'KAUSHAL'\", \"b'HAMMER'\", \"b'SAVE'\", \"b'WWWWEBSCOM'\", \"b'KNIGHT'\", \"b'RESULTS'\", \"b'TAYLOR'\", \"b'STATE'\", \"b'SIP'\", \"b'LANE'\", \"b'FUTURE'\", \"b'FROM'\", \"b'CRESTDOWN'\", \"b'REMITTANCES'\", \"b'THE'\", \"b'FAX'\", \"b'RECRUITMENT'\", \"b'37'\", \"b'12530'\", \"b'FANTASTISK'\", \"b'DR'\", \"b'INDIA'\", \"b'GC'\", \"b'FERRABY'\", \"b'DOGS'\", \"b'EVERY'\", \"b'PERSEEL'\", \"b'YOUR'\", \"b'VILLA'\", \"b'CAMPAIGN'\", \"b'FARM'\", \"b'ALL'\", \"b'5'\", \"b'FOX'\", \"b'BANK'\", \"b'HOFFMAN'\", \"b'AS'\", \"b'MART'\", \"b'ROZOWEJ'\", \"b'SDN'\", \"b'ZEROEMISSIONS'\", \"b'GREM'\", \"b'FOR'\", \"b'DEPOSITPHOTOS'\", \"b'1300'\", \"b'WILT'\", \"b'CAREER'\", \"b'360'\", \"b'COPIES'\", \"b'COPY'\", \"b'OF'\", \"b'TRINITYLEEDS'\", \"b'EN'\", \"b'UP'\", \"b'ONGC'\", \"b'DANGERS'\", \"b'CAUTION'\", \"b'REPORT'\", \"b'CANNIESBURN'\", \"b'NEW'\", \"b'MADE'\", \"b'MEDICAL'\", \"b'KIRKLISTON'\", \"b'PERSONAL'\", \"b'LIVING'\", \"b'FRANKLIN'\", \"b'HOW'\", \"b'8401950'\", \"b'END'\", \"b'ANDRIEJ'\", \"b'DO'\", \"b'KEENER'\", \"b'THEM'\", \"b'NAME'\", \"b'LANE'\", \"b'TRACK'\", \"b'CHRYSLER'\", \"b'SCHWARZENEGGER'\", \"b'ONE'\", \"b'ALL'\", \"b'1306'\", \"b'BRASSERIE'\", \"b'OR'\", \"b'WHEN'\", \"b'ROADS'\", \"b'INDIA'\", \"b'THROUGHOUT'\", \"b'DO'\", \"b'8'\", \"b'2'\", \"b'DAY'\", \"b'137'\")\n"
     ]
    }
   ],
   "source": [
    "# texts is tuple, each element is a string label. 100 in total\n",
    "import collections\n",
    "print(isinstance(texts, collections.Iterable))\n",
    "new_texts = [s[2:-1] for s in texts]\n",
    "print(new_texts)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 32, 100])\n"
     ]
    }
   ],
   "source": [
    "# check batch image size\n",
    "print(images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert image and label to Torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine all image to wanted size: (32,100)\n",
    "# return v\n",
    "def refine_img(v, img):\n",
    "    v.data.resize_(img.size()).copy_(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store batch image and text in Tensor\n",
    "image = torch.FloatTensor(100, 1, 32, 100)  # 100: batch size  imgH, imgW\n",
    "text = torch.IntTensor(100 * 5)\n",
    "batch_size = torch.IntTensor(100)\n",
    "\n",
    "image = Variable(image)\n",
    "text = Variable(text)\n",
    "batch_size = Variable(batch_size)\n",
    "\n",
    "# TEST\n",
    "refine_img(image, images)  # images: read from train_iterator\n",
    "#print(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to encode and decode the label\n",
    "# Example:\n",
    "# Encode: 7abc --> [7,10,11,12]  (IntTensor)\n",
    "# Docode: [11,12,13] --> [b,c,d]\n",
    "# *****TODO*****\n",
    "# wrap all functions in a CLASS\n",
    "\n",
    "# Define alphabet in config.py\n",
    "alphabet = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "class LabelConverter(object):\n",
    "    def __init__(self, alphabet, ignore_case = True):\n",
    "        self.ignore_case = ignore_case\n",
    "        if self.ignore_case:\n",
    "            alphabet = alphabet.lower()\n",
    "        self.alphabet = alphabet + '-'\n",
    "        self.dict = {}\n",
    "        for index, c in enumerate(self.alphabet):\n",
    "            self.dict[c] = index + 1  # index 0 reserved for blank\n",
    "    \n",
    "    def encode(self, labels):\n",
    "        # input: labels: list of labels name\n",
    "        # return: [labels encoded], [labels length]\n",
    "        length = [len(c) for c in labels]\n",
    "        text = ''.join(labels)\n",
    "        encoded = []\n",
    "        for c in text:\n",
    "            encoded.append(self.dict[c.lower()] )\n",
    "        return torch.IntTensor(encoded), torch.IntTensor(length)\n",
    "    \n",
    "    def decode(self, text, length):\n",
    "        # decode to strs, batch mode\n",
    "        # text: list of encodings    length: list of length\n",
    "        assert sum(length) == text.numel()  # .numel used to calculate number of elements in text\n",
    "        decodings = []\n",
    "        pos = 0\n",
    "        for str_len in length:\n",
    "            encode = text[pos:pos+str_len]\n",
    "            decode = ''\n",
    "            for digit in encode:\n",
    "                decode += self.alphabet[digit-1]\n",
    "            decodings.append(decode)\n",
    "            pos+= str_len\n",
    "        return decodings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([530])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "labelConverter = LabelConverter(alphabet)\n",
    "encode_label, encode_length = labelConverter.encode(new_texts)\n",
    "encode_label.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Manage Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new a network object and initialize its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class RCNN definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BinLSTM, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, imgW, n_hidden = 2, n_class=36, init_weights=True):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cfg = [64, 'M22', 128, 'M22', 256, 256, 'M12', 512, 'bc', 512, 'bc', 'M12', 512]\n",
    "        self.out_channels = [64, 128, 256, 256, 512, 512, 512]  # output channels of each conv layers\n",
    "        self.layers = self.make_layers(self.cfg)\n",
    "        self.rnn = nn.Sequential(\n",
    "            BinLSTM(512, n_hidden, n_hidden),\n",
    "            BinLSTM(n_hidden, n_hidden, n_class))\n",
    "        \n",
    "    def make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 1  # in this example, channel is 1\n",
    "        layer_count = -1  # count conv layers\n",
    "        for v in cfg:\n",
    "            if v == 'M22':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif v == 'M12':\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,1), padding=(0,1))]\n",
    "            elif v == 'bc':\n",
    "                layers += [nn.BatchNorm2d(self.out_channels[layer_count]), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                if layer_count == 5:\n",
    "                    conv2d = nn.Conv2d(in_channels, v, kernel_size=2, padding=0)\n",
    "                else:\n",
    "                    conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                layer_count += 1\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        b,c,h,w = x.size()\n",
    "        # print(b,c,h,w)  100,512,1,26\n",
    "        # convert it to   26,100,512\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        x = x.squeeze(2) # squeeze second dimension --> 100 512 26\n",
    "        x = x.permute(2, 0, 1)  # 26 100 512\n",
    "        # input to the lstm MUST be 3D: \n",
    "        # The first axis is the sequence itself, the second indexes mini-batch size\n",
    "        x = self.rnn(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST network\n",
    "crnn = CRNN(32, 100)\n",
    "output = crnn(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 100, 36])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-e99a54a2f558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TEST network\n",
    "test_tensor = torch.randn(100, 1, 32, 100)\n",
    "\n",
    "texts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
